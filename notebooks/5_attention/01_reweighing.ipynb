{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweighing based on proximity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get some simple timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(0, 4 * torch.pi, 100)\n",
    "\n",
    "\n",
    "def f(t):\n",
    "    noise = torch.randn(100) * 0.1\n",
    "    return torch.sin(t) + noise\n",
    "\n",
    "\n",
    "x = f(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create a filter that smooths the signal by taking in the neighbouring signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a convolution\n",
    "conv = torch.nn.Conv1d(1, 1, 8)\n",
    "\n",
    "# build a gaussian kernel\n",
    "n = torch.distributions.normal.Normal(0, 1)\n",
    "v = torch.arange(-4, 4)\n",
    "gaussian = torch.exp(n.log_prob(v))[None, None, :]\n",
    "\n",
    "# replace the random weights\n",
    "d = conv.state_dict()\n",
    "d[\"weight\"] = gaussian\n",
    "conv.load_state_dict(d)\n",
    "x_ = conv(x[None, None, :])[0][0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot everything like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(t, x)\n",
    "ax[0].set_title(\"original\")\n",
    "\n",
    "ax[1].scatter([*range(8)], gaussian[0][0])\n",
    "ax[1].set_title(\"gaussian filter\")\n",
    "\n",
    "ax[2].plot(x_.detach())\n",
    "ax[2].set_title(\"smoothed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you see a noisy signal. The gaussian filter takes adds more of the close values, and less of the values further away. The filter works on the direct neighborhood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we consider text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace() # This pre-tokenizer simply splits using the following regex: \\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"ik zit op de bank\", \"ik werk bij de bank\", \"bij de bank is het heel druk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the problem with text is: it is not necessarily the words that are close, that have the most impact."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see three sentences, max seven words, so dimensions are (3,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.encode(corpus[0])\n",
    "x.ids\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sentences are being encoded, and the word \"bank\" gets the integer 6 assigned. However, the meaning of this word is not the same because of the context... If we make an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "vocab_len = tokenizer.get_vocab_size()\n",
    "\n",
    "emb = nn.Embedding(num_embeddings=vocab_len, embedding_dim=4, padding_idx=0)\n",
    "\n",
    "tokenized_corpus = pad_sequence([torch.tensor(tokenizer.encode(s).ids) for s in corpus], batch_first=True)\n",
    "\n",
    "embeddings = emb(tokenized_corpus)\n",
    "\n",
    "embeddings, embeddings.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added a dimensionality of 4 to every word. So now we have (3, 7, 4).\n",
    "You can see that the word \"bank\" gets exactly the same vector, as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank1 = embeddings[0][4]\n",
    "bank2 = embeddings[1][4]\n",
    "bank1, bank2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "Now we will start with the attention mechanism.\n",
    "We need a key, query and value. Because we use self attention, these are just clones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = embeddings.detach().clone()\n",
    "query = embeddings.detach().clone()\n",
    "values = embeddings.detach().clone()\n",
    "key.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_features = torch.tensor(query.shape[-1])\n",
    "d_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this, we can calculate $$\\frac{(QK^T)}{\\sqrt{d}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.bmm(query, key.transpose(1, 2)) / torch.sqrt(d_features)\n",
    "dots.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a shape of (3, 7, 7):\n",
    "for every sentence, we have for every word, weights how we want to mix in every other word. So this last part always has a shape (sequence, sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the weights with a softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = nn.Softmax(dim=-1)(dots)\n",
    "\n",
    "weights[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally we can do a matrix-multiplication with the values:\n",
    "\n",
    "$$attention = softmax\\left(\\frac{(QK^T)}{\\sqrt{d}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = torch.bmm(weights, values)\n",
    "activations.shape, embeddings.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note how we end up with exactly the same size: 3 sentences, max 7 words, but now every word has 4 dimensions that are reweighted by the other words in the sentence, regardless of the distance, but mainly depending on the semantics (meaning) of every word as encoded in de embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank1 = activations[0][4]\n",
    "bank2 = activations[1][4]\n",
    "bank1, bank2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the vector for the word bank has been \"mixed\" with all the other words in the sentence, and they are different!\n",
    "\n",
    "torch has a multihead attention implemented. With that, we can add a mask to cover the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tokenized_corpus == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead = nn.MultiheadAttention(embed_dim=4, num_heads=2, batch_first=True)\n",
    "attn, attn_w = multihead(query, key, values, key_padding_mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to visualize the weights. In this case, this is untrained.\n",
    "What you expect is that after training the vector for the word \"bank\" should be mostly mixed with the word \"zit\" (sit) to make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = corpus[0].split()\n",
    "labels = labels + [\"PAD\", \"PAD\"]\n",
    "\n",
    "plot = sns.heatmap(attn_w[0].detach().numpy())\n",
    "\n",
    "plot.set_xticklabels(labels);\n",
    "plot.set_yticklabels(labels);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
